{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szTAf-DSVPc4",
        "outputId": "7015eed7-6b94-4113-aab5-a2111ea12cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov  9 19:10:14 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas tensorflow scikit-learn tqdm\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "z-kwk-twVXIq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbtCpdVaVgVm",
        "outputId": "bdd4c802-d260-49c1-82b3-b86f04c95f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/cs challenge/cs challenge/NSL-KDD-Dataset-master/'"
      ],
      "metadata": {
        "id": "coasO-9oVknH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swJsry9uUxD-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.73    # discount factor (from paper)\n",
        "        self.epsilon = 0.5   # exploration rate (from paper)\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.5  # from paper\n",
        "        self.model = self._build_model()\n",
        "        self.attack_therapy = {\n",
        "            0: \"DoS\",   # Denial of Service\n",
        "            1: \"Probe\",  # Probe\n",
        "            2: \"U2R\",    # User to Root\n",
        "            3: \"R2L\"     # Remote to Local\n",
        "        }\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return  # Do not perform replay if there aren't enough experiences in memory\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += self.gamma * np.max(self.model.predict(next_state))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "\n",
        "    def apply_therapy(self, action):\n",
        "        # Translate action to attack therapy\n",
        "        attack_type = self.attack_therapy.get(action, \"Unknown\")\n",
        "\n",
        "        # Apply firewall therapy based on attack type\n",
        "        if attack_type == \"DoS\":\n",
        "            self.dos_therapy()\n",
        "        elif attack_type == \"Probe\":\n",
        "            self.probe_therapy()\n",
        "        elif attack_type == \"U2R\":\n",
        "            self.u2r_therapy()\n",
        "        elif attack_type == \"R2L\":\n",
        "            self.r2l_therapy()\n",
        "      #### we work with coll So we just print the actions that our model do ###\n",
        "    def dos_therapy(self):\n",
        "        \"\"\"Increase firewall rate limiting and block IP\"\"\"\n",
        "        print(\"Applying DoS Therapy: Increasing rate limiting and blocking IP\")\n",
        "        print(\"command = New-NetFirewallRule -DisplayName 'DoS Block' -Direction Inbound -Action Block -Protocol TCP -LocalPort 80\")\n",
        "        print(\"self._execute_powershell_command(command)\")\n",
        "\n",
        "    def probe_therapy(self):\n",
        "        \"\"\"Enable stricter logging and detection\"\"\"\n",
        "        print(\"Applying Probe Therapy: Enabling stricter logging and detection\")\n",
        "        print(\"command = Set-NetFirewallProfile -All -LogFileName 'C:\\\\FirewallLogs\\\\firewall.log' -LogMaxSize 4096\")\n",
        "        print(\"self._execute_powershell_command(command)\")\n",
        "\n",
        "    def u2r_therapy(self):\n",
        "        \"\"\"Isolate affected system and reset root password\"\"\"\n",
        "        print(\"Applying U2R Therapy: Isolating affected system and resetting root password\")\n",
        "        # Simulating system isolation by blocking IP and resetting root password (not realistic in all systems)\n",
        "        print(\"command_block = New-NetFirewallRule -DisplayName 'U2R Block' -Direction Inbound -Action Block -RemoteAddress 'affected_ip'\")\n",
        "        print(\"command_reset = net user Administrator /active:no\")  # Deactivating Administrator account\n",
        "        print(\"self._execute_powershell_command(command_block)\")\n",
        "        print(\"self._execute_powershell_command(command_reset)\")\n",
        "\n",
        "    def r2l_therapy(self):\n",
        "        \"\"\"Strengthen login security and monitor unauthorized access\"\"\"\n",
        "        print(\"Applying R2L Therapy: Strengthening login security and monitoring unauthorized access\")\n",
        "        print(\"command_2fa = Set-LocalUser -Name 'Administrator' -PasswordNeverExpires $true\")\n",
        "        print(\"command_monitoring = Set-NetFirewallProfile -All -AllowInboundRemoteDesktop $false\")  # Blocking remote access\n",
        "        print(\"self._execute_powershell_command(command_2fa)\")\n",
        "        print(\"self._execute_powershell_command(command_monitoring)\")\n",
        "\n",
        "    def _execute_powershell_command(self, command):\n",
        "        \"\"\"Execute PowerShell command from Python\"\"\"\n",
        "        print(\"process = subprocess.Popen([powershell, -Command, command], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\")\n",
        "        print(\"out, err = process.communicate()\")\n",
        "        #if err:\n",
        "           # print(f\"Error: {err.decode()}\")\n",
        "       # else:\n",
        "           # print(f\"Output: {out.decode()}\")\n",
        "\n",
        "def preprocess_data(file_path):\n",
        "    # Read the NSL-KDD dataset\n",
        "    columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
        "               'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
        "               'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
        "               'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
        "               'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
        "               'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
        "               'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
        "               'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
        "               'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "               'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "               'dst_host_srv_rerror_rate', 'class', 'extra_feature']\n",
        "\n",
        "    df = pd.read_csv(file_path, names=columns)\n",
        "\n",
        "    # Convert categorical variables to numeric\n",
        "    categorical_columns = ['protocol_type', 'service', 'flag']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop(['class', 'extra_feature'], axis=1)\n",
        "    y = df['class']\n",
        "\n",
        "    # Convert labels to binary (normal=0, attack=1)\n",
        "    y = (y != 'normal').astype(int)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return X_scaled, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # For tracking time\n",
        "from tqdm import tqdm  # For a progress bar (optional)\n",
        "\n",
        "def train_dqn(X_train, y_train, episodes=1):\n",
        "    state_size = X_train.shape[1]\n",
        "    action_size = 2  # fix or escalate\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    batch_size = 8 #############batch size  a changer 8 / 16\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        total_reward = 0\n",
        "        correct_predictions = 0\n",
        "\n",
        "        # Add a progress bar here  800 a changer vers toute la data\n",
        "        with tqdm(total=800, desc=f\"Episode {episode+1}/{episodes}\", unit=\"sample\") as pbar:\n",
        "            for i in range(800):  # Train on only 200 samples\n",
        "                start_time = time.time()  # Start time for each sample\n",
        "\n",
        "                state = X_train[i].reshape(1, -1)\n",
        "                action = agent.act(state)\n",
        "                reward = 1 if action == y_train[i] else 0\n",
        "\n",
        "                # For the last state, we'll use the same state as next_state\n",
        "                next_state = X_train[i+1].reshape(1, -1) if i < 199 else state\n",
        "                done = i == 199\n",
        "\n",
        "                agent.remember(state, action, reward, next_state, done)\n",
        "                total_reward += reward\n",
        "\n",
        "                if action == y_train[i]:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "                if len(agent.memory) > batch_size:\n",
        "                    agent.replay(batch_size)\n",
        "\n",
        "                # Measure time elapsed for the current sample\n",
        "                sample_time = time.time() - start_time\n",
        "\n",
        "                # Print details for each sample (optional, can be removed for brevity)\n",
        "                print(f\"Episode: {episode}, Sample: {i}/{800}, Accuracy: {correct_predictions/(i+1):.4f}, Reward: {reward}, Time per Sample: {sample_time:.4f} sec\")\n",
        "\n",
        "                pbar.update(1)  # Update progress bar\n",
        "\n",
        "        accuracy = correct_predictions / 800\n",
        "        results.append({\n",
        "            'episode': episode,\n",
        "            'accuracy': accuracy,\n",
        "            'total_reward': total_reward\n",
        "        })\n",
        "        print(f\"Episode: {episode}, Accuracy: {accuracy:.4f}, Total Reward: {total_reward}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    agent.model.save('dqn_model_800_samples.keras')\n",
        "\n",
        "    return agent, results\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess training data\n",
        "    X_train, y_train = preprocess_data(path+'KDDTrain+.txt')\n",
        "\n",
        "    # Train the model on only 200 samples\n",
        "    agent, results = train_dqn(X_train[:800], y_train[:800], episodes=7)  # Train on only the first 200 samples\n",
        "\n",
        "    # Convert results to DataFrame for easy analysis\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\nTraining Results:\")\n",
        "    print(results_df)\n"
      ],
      "metadata": {
        "id": "H_VNvuSuWlgB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zns4LCoqWyGl"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}